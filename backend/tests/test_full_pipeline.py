#!/usr/bin/env python3
"""
Test Full SEO Analysis Pipeline
é€æ­¥æµ‹è¯•æ¯ä¸ªç»„ä»¶ï¼Œç”Ÿæˆä¸­é—´ç»“æœæ–‡ä»¶
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent
sys.path.insert(0, str(project_root))

print(f"DEBUG: è„šæœ¬è·¯å¾„: {__file__}")
print(f"DEBUG: é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print(f"DEBUG: Pythonè·¯å¾„: {sys.path}")

try:
    from app.services.seo_analysis_service import SEOAnalysisService
    from app.core.lhr_parser import LHRTool
    from app.core.matcher import match_issues
    from app.core.issue_merger import transform_to_simple_issues_with_insertions
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"Pythonè·¯å¾„: {sys.path}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    sys.exit(1)

def save_json_file(data, filename, description):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    output_dir = Path(__file__).parent / "pipeline_outputs"
    output_dir.mkdir(exist_ok=True)
    
    filepath = output_dir / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… {description} å·²ä¿å­˜åˆ°: {filepath}")
    return filepath

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„SEOåˆ†ææµç¨‹"""
    
    # 1. è¯»å–æµ‹è¯•HTMLæ–‡ä»¶
    print("ğŸ“– è¯»å–æµ‹è¯•HTMLæ–‡ä»¶...")
    html_file_path = Path(__file__).parent / "test_seo_page.html"
    
    if not html_file_path.exists():
        print(f"âŒ HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_file_path}")
        return
    
    with open(html_file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f"âœ… HTMLæ–‡ä»¶è¯»å–æˆåŠŸï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
    
    # 2. è°ƒç”¨LighthouseæœåŠ¡
    print("\nğŸ” æ­¥éª¤1: è°ƒç”¨LighthouseæœåŠ¡...")
    try:
        seo_service = SEOAnalysisService()
        lighthouse_result = seo_service._call_lighthouse_service(html_content)
        
        # ä¿å­˜LighthouseåŸå§‹ç»“æœ
        save_json_file(
            lighthouse_result, 
            "01_lighthouse_raw.json", 
            "LighthouseåŸå§‹ç»“æœ"
        )
        
        print(f"âœ… LighthouseæœåŠ¡è°ƒç”¨æˆåŠŸï¼ŒSEOåˆ†æ•°: {lighthouse_result.get('seoScore', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ LighthouseæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
        return
    
    # 3. è¿è¡ŒLHRè§£æå™¨
    print("\nğŸ“Š æ­¥éª¤2: è¿è¡ŒLHRè§£æå™¨...")
    try:
        parser = LHRTool()
        parsed_result = parser.parse_lhr_json(lighthouse_result)
        
        # ä¿å­˜è§£æåçš„ç»“æœ
        save_json_file(
            parsed_result, 
            "02_parser_output.json", 
            "LHRè§£æå™¨è¾“å‡º"
        )
        
        print(f"âœ… è§£æå™¨è¿è¡ŒæˆåŠŸï¼Œé—®é¢˜æ•°é‡: {len(parsed_result.get('issues', []))}")
        
    except Exception as e:
        print(f"âŒ è§£æå™¨è¿è¡Œå¤±è´¥: {e}")
        return
    
    # 4. è¿è¡ŒåŒ¹é…å™¨
    print("\nğŸ¯ æ­¥éª¤3: è¿è¡ŒåŒ¹é…å™¨...")
    try:
        matched_result = match_issues(html_content, parsed_result)
        
        # ä¿å­˜åŒ¹é…ç»“æœ
        save_json_file(
            matched_result, 
            "04_matcher_output.json", 
            "åŒ¹é…å™¨è¾“å‡º"
        )
        
        matched_issues = matched_result.get('issues', [])
        matched_count = len([i for i in matched_issues if i.get('match_status') == 'matched'])
        print(f"âœ… åŒ¹é…å™¨è¿è¡ŒæˆåŠŸï¼Œæ€»é—®é¢˜: {len(matched_issues)}, å·²åŒ¹é…: {matched_count}")
        
    except Exception as e:
        print(f"âŒ åŒ¹é…å™¨è¿è¡Œå¤±è´¥: {e}")
        return
    
    # 5. è¿è¡Œé—®é¢˜åˆå¹¶å™¨
    print("\nğŸ”§ æ­¥éª¤4: è¿è¡Œé—®é¢˜åˆå¹¶å™¨...")
    try:
        merged_issues = transform_to_simple_issues_with_insertions(matched_result)
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        save_json_file(
            merged_issues, 
            "04_merger_output.json", 
            "é—®é¢˜åˆå¹¶å™¨è¾“å‡º"
        )
        
        print(f"âœ… é—®é¢˜åˆå¹¶å™¨è¿è¡ŒæˆåŠŸï¼Œåˆå¹¶åé—®é¢˜æ•°é‡: {len(merged_issues)}")
        
    except Exception as e:
        print(f"âŒ é—®é¢˜åˆå¹¶å™¨è¿è¡Œå¤±è´¥: {e}")
        return
    
    # 6. æ„å»ºæœ€ç»ˆç»“æœ
    print("\nğŸ“ æ­¥éª¤5: æ„å»ºæœ€ç»ˆç»“æœ...")
    try:
        final_result = seo_service._build_final_result(
            parsed_result, 
            merged_issues, 
            html_content
        )
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        if hasattr(final_result, 'dict'):
            final_dict = final_result.dict()
        else:
            final_dict = final_result.__dict__
        
        save_json_file(
            final_dict, 
            "05_final_result.json", 
            "æœ€ç»ˆSEOåˆ†æç»“æœ"
        )
        
        print(f"âœ… æœ€ç»ˆç»“æœæ„å»ºæˆåŠŸï¼ŒSEOåˆ†æ•°: {final_dict.get('seo_score', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆç»“æœæ„å»ºå¤±è´¥: {e}")
        return
    
    print("\nğŸ‰ å®Œæ•´æµç¨‹æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ æ‰€æœ‰ä¸­é—´ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° tests/pipeline_outputs/ ç›®å½•")
    print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  01_lighthouse_raw.json      - LighthouseåŸå§‹ç»“æœ")
    print("  02_parser_output.json       - LHRè§£æå™¨è¾“å‡º")
    print("  03_matcher_output.json      - åŒ¹é…å™¨è¾“å‡º")
    print("  04_merger_output.json       - é—®é¢˜åˆå¹¶å™¨è¾“å‡º")
    print("  05_final_result.json        - æœ€ç»ˆSEOåˆ†æç»“æœ")

if __name__ == "__main__":
    test_full_pipeline()
