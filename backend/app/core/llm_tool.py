# LLM Tool - Core utility for language model interactions
import requests
import json

from typing import List

from ..config import Config

from app.schemas.seo_analysis import SEOAnalysisResult

class LLMTool:
    """
    Core tool for interacting with Google's Generative AI API
    """
    
    def __init__(self):
        self.url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        self.api_key = Config.GOOGLE_API_KEY
        self.validate_config()
    
    def validate_config(self):
        """Validate that required configuration is present"""
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
    
    def generate_content(self, prompt: str) -> str:
        """
        Generate content using Google's Generative AI API (Gemini)
        """
        try:
            # Use the correct Google Generative AI API endpoint and headers
            
            headers = {
                "Content-Type": "application/json",
                "X-goog-api-key": self.api_key
            }
            print("prompt: ", prompt)
            
            payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ]
            }
            print("making call to gemini...")
            response = requests.post(self.url, headers=headers, json=payload)
            
            if response.status_code != 200:
                response.raise_for_status()
            
            result = response.json()
            
            # Extract the generated content
            if "candidates" in result and len(result["candidates"]) > 0:
                generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
                return generated_text.strip()
            else:
                raise Exception("No content generated by LLM")
                
        except Exception as e:
            raise e 
    
    # gets the html content
    def get_modification(self, modify_context: str, match_html: str, context: str) -> str:

        prompt = (
            f"Fix this HTML snippet for SEO:\n\n"
            f"Original: {match_html}\n"
            f"Issue: {modify_context}\n\n"
            f"Context: {context}\n\n"
            "Return ONLY the corrected HTML without markdown"
        )

        optimized_html = self.generate_content(prompt)
        return optimized_html
    
    # Process issues in batches to reduce context repetition
    def get_batch_modification(self, analysis_res: SEOAnalysisResult) -> SEOAnalysisResult:
        issues_list = analysis_res.issues
        batch_size = 10  # Process 10 issues per batch
        
        # Process issues in batches
        for i in range(0, len(issues_list), batch_size):
            batch = issues_list[i:i+batch_size]
            self.process_batch(batch, analysis_res.context, i//batch_size + 1)
        
        return analysis_res

    def process_batch(self, batch_issues: List[IssueInfo], context: str, batch_num: int):
        """Process a batch of issues, sending context only once per batch"""
        
        # Build prompt containing 10 issues with very explicit output requirements
        batch_prompt = f"""
Page Context:
{context}

You must fix exactly {len(batch_issues)} SEO issues. Return EXACTLY {len(batch_issues)} fixes in this EXACT format:

ISSUE_1: [fixed HTML]
ISSUE_2: [fixed HTML]
ISSUE_3: [fixed HTML]
...
ISSUE_{len(batch_issues)}: [fixed HTML]

IMPORTANT RULES:
- You MUST return exactly {len(batch_issues)} fixes
- Each fix must start with "ISSUE_X:" followed by the corrected HTML
- Return ONLY the corrected HTML, no explanations or markdown
- Each fix must be on a separate line
- The HTML must be valid and complete

Issues to fix:
"""
        
        # Add information for each issue with clear numbering
        for i, issue in enumerate(batch_issues, 1):
            batch_prompt += f"\nISSUE_{i}:\nTitle: {issue.title}\nHTML: {issue.raw_html}\n"
        
        # Call LLM
        response = self.generate_content(batch_prompt)
        
        # Parse response and assign to each issue
        self.parse_batch_response(response, batch_issues)

    def parse_batch_response(self, response: str, batch_issues: List[IssueInfo]):
        """Parse batch response and assign fixes to each issue with robust error handling"""
        
        # Clean and normalize the response
        response = response.strip()
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        
        # Track which issues we've successfully parsed
        parsed_issues = set()
        
        for i, issue in enumerate(batch_issues, 1):
            issue_marker = f"ISSUE_{i}:"
            found_fix = False
            
            # Look for the exact issue marker
            for j, line in enumerate(lines):
                if line.startswith(issue_marker):
                    # Extract HTML content after the marker
                    html_content = line[len(issue_marker):].strip()
                    
                    # If HTML is incomplete on this line, collect from subsequent lines
                    if not html_content.startswith('<') or not html_content.endswith('>'):
                        html_lines = [html_content]
                        # Collect HTML from next lines until we hit another ISSUE marker or empty line
                        for k in range(j + 1, len(lines)):
                            next_line = lines[k]
                            if next_line.startswith('ISSUE_') or next_line == '':
                                break
                            html_lines.append(next_line)
                        
                        html_content = ' '.join(html_lines).strip()
                    
                    # Validate that we got actual HTML
                    if html_content.startswith('<') and html_content.endswith('>'):
                        issue.optimized_html = html_content
                        parsed_issues.add(i)
                        found_fix = True
                        print(f"‚úÖ Successfully parsed ISSUE_{i}")
                        break
                    else:
                        print(f"‚ö†Ô∏è  Invalid HTML for ISSUE_{i}: {html_content[:100]}...")
            
            # If no valid fix found, keep original HTML and log warning
            if not found_fix:
                print(f"‚ùå No valid fix found for ISSUE_{i}, keeping original HTML")
                issue.optimized_html = issue.raw_html
        
        # Summary of parsing results
        success_count = len(parsed_issues)
        total_count = len(batch_issues)
        print(f"üìä Batch parsing complete: {success_count}/{total_count} issues successfully parsed")
        
        if success_count < total_count:
            print(f"‚ö†Ô∏è  Warning: {total_count - success_count} issues could not be parsed properly")
            print("Response received:", response[:500] + "..." if len(response) > 500 else response) 
    
    