# LLM Tool - Core utility for language model interactions
import requests
import json

from typing import List

from ..config import Config

from app.schemas.seo_analysis import SEOAnalysisResult

class LLMTool:
    """
    Core tool for interacting with Google's Generative AI API
    """
    
    def __init__(self):
        self.url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        self.api_key = Config.GOOGLE_API_KEY
        self.validate_config()
    
    def validate_config(self):
        """Validate that required configuration is present"""
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
    
    def generate_content(self, prompt: str) -> str:
        """
        Generate content using Google's Generative AI API (Gemini)
        """
        try:
            # Use the correct Google Generative AI API endpoint and headers
            
            headers = {
                "Content-Type": "application/json",
                "X-goog-api-key": self.api_key
            }
            print("prompt: ", prompt)
            
            payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ]
            }
            print("making call to gemini...")
            response = requests.post(self.url, headers=headers, json=payload)
            
            if response.status_code != 200:
                response.raise_for_status()
            
            result = response.json()
            
            # Extract the generated content
            if "candidates" in result and len(result["candidates"]) > 0:
                generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
                return generated_text.strip()
            else:
                raise Exception("No content generated by LLM")
                
        except Exception as e:
            raise e 
    
    # gets the html content
    def get_modification(self, modify_context: str, match_html: str) -> str:

        prompt = (
            f"Fix this HTML snippet for SEO:\n\n"
            f"Original: {match_html}\n"
            f"Issue: {modify_context}\n\n"
            "Return ONLY the corrected HTML without markdown"
        )

        optimized_html = self.generate_content(prompt)
        return optimized_html
    
    # loop through all the things that needs to be modified and calls get_modification
    def get_batch_modification(self, analysis_res: SEOAnalysisResult) -> SEOAnalysisResult:
        issues_list = analysis_res.issues
        
        for issue in issues_list:
            optimized = self.get_modification(issue.title, issue.raw_html)
            issue.optimized_html = optimized
        
        return analysis_res
    
    