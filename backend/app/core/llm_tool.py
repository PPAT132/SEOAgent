# LLM Tool - Core utility for language model interactions
import requests
import json
from ..config import Config

class LLMTool:
    """
    Core tool for interacting with Google's Generative AI API
    """
    
    def __init__(self):
        print("DEBUG: Initializing LLMTool")
        self.api_key = Config.GOOGLE_API_KEY
        print(f"DEBUG: API key loaded: {'Yes' if self.api_key else 'No'}")
        if self.api_key:
            print(f"DEBUG: API key preview: {self.api_key[:10]}...")
        self.validate_config()
        print("DEBUG: LLMTool validation passed")
    
    def validate_config(self):
        """Validate that required configuration is present"""
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
    
    def generate_content(self, prompt: str) -> str:
        """
        Generate content using Google's Generative AI API (Gemini)
        """
        print(f"DEBUG: LLMTool.generate_content called with prompt length: {len(prompt)}")
        
        try:
            # Use the correct Google Generative AI API endpoint and headers
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
            
            headers = {
                "Content-Type": "application/json",
                "X-goog-api-key": self.api_key
            }
            
            payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ]
            }
            
            print(f"DEBUG: Making request to Google Generative AI API with gemini-2.0-flash model")
            response = requests.post(url, headers=headers, json=payload)
            print(f"DEBUG: Response status code: {response.status_code}")
            
            if response.status_code != 200:
                print(f"DEBUG: Response text: {response.text}")
            
            response.raise_for_status()
            
            result = response.json()
            print(f"DEBUG: Response JSON keys: {list(result.keys())}")
            
            # Extract the generated content
            if "candidates" in result and len(result["candidates"]) > 0:
                generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
                print(f"DEBUG: Generated text length: {len(generated_text)}")
                return generated_text.strip()
            else:
                print(f"DEBUG: No candidates in response: {result}")
                raise Exception("No content generated by LLM")
                
        except Exception as e:
            print(f"DEBUG: LLM generation failed: {str(e)}")
            raise e 